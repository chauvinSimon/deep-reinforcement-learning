{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor progress - flush espilon\n",
    "print(\"\\rEpisode {}/{} || Best average reward {} - agent.epsilon = {}\".format(i_episode, num_episodes, best_avg_reward, agent.epsilon), end=\"\")\n",
    "sys.stdout.flush()\n",
    "# or\n",
    "print(\"\\r {}, {}, {}, {}, {}\".format(action, reward, done, min_reward, max_reward),sep=' ', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TD - avg_scores (window) with deque\n",
    "from collections import deque\n",
    "    # monitor performance\n",
    "    tmp_scores = deque(maxlen=plot_every)     # deque for keeping track of scores\n",
    "    avg_scores = deque(maxlen=num_episodes)   # average scores over every plot_every episodes\n",
    "    score = 0\n",
    "    \n",
    "    # in the loop\n",
    "    score += reward                                   # add reward to agent's score\n",
    "    # when terminated\n",
    "    tmp_scores.append(score)    # append score\n",
    "    if (i_episode % plot_every == 0):\n",
    "        avg_scores.append(np.mean(tmp_scores))\n",
    "\n",
    "# plot performance\n",
    "plt.plot(np.linspace(0,num_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "plt.show()\n",
    "# print best 100-episode performance\n",
    "print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Value Function\n",
    "# first convert Q to V\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])\n",
    "print(V_sarsa)\n",
    "# For a specific state (e.g. the one just before the end), we know the value (= Reward)\n",
    "expected_V_last_but_one = +10\n",
    "print(V_sarsa[last_but_one] vs expected_V_last_but_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 0: 3, -1: 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "actions = [1, 2, 0, 1, 0, 1]\n",
    "rewards = [+1, 0, 0, 0, 1, -1]\n",
    "state = np.array([0, 1, 2])\n",
    "next_state = np.array([0, 1, 0])\n",
    "changes_in_state = 0\n",
    "from collections import Counter\n",
    "# monitor if the agent moves\n",
    "if not (next_state == state).all():\n",
    "    changes_in_state = changes_in_state + 1  # changes_in_state: 300\n",
    "# monitor distribution in actions\n",
    "Counter(actions)  # action: Counter({1: 151, 0: 149}) shows tilts\n",
    "# monitor distribution in rewards\n",
    "Counter(rewards)  # rewards: Counter({0.0: 298, -1.0: 1, 1.0: 1}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have noticed it progressively learns to tilt\n",
    "# Clearly visible on the scene (it does not move)\n",
    "# And in the distribution of actions I record: \t action: Counter({1: 151, 0: 149})\n",
    "# Should not it be possible to mask action accordingly\n",
    "# For instance, from the beginning, disable the backward step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is the state built?\n",
    "# I understood some laser measurements are used to estimate distance in several directions.\n",
    "# But how (if applicable) are encoded the following aspects:\n",
    "# - Colour of the bananas?\n",
    "# - Previous action?\n",
    "# - Previous state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linspace() - not np.arange()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "eps_decay = 0.995\n",
    "nb_episodes = 1500\n",
    "eps_end = 0.01\n",
    "eps_start = 1.0\n",
    "eps = [max(eps_start * eps_decay ** i, eps_end) for i in range(nb_episodes)]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(eps)), eps)\n",
    "plt.plot(np.arange(len(eps)), np.linspace(eps_start, eps_end, len(eps)))\n",
    "plt.ylabel('Espilon')\n",
    "plt.xlabel('Episode #')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Notes\n",
    "\n",
    "- Try **normalizing your future rewards** over all the **parallel agents**, it can speed up training\n",
    "- **Simpler networks** might perform better than more complicated ones! The original input contains 80x80x2=12800 numbers, you might want to **ensure that this number steadily decreases** at each layer of the neural net.\n",
    "- Training performance may be significantly worse on local machines. I had worse performance training on my own windows desktop with a 4-core CPU and a GPU. This may be due to the slightly different ways the emulator is rendered. So please run the code on the workspace first before moving locally\n",
    "- It may be beneficial to **train multiple epochs**\n",
    " - say first using a small tmax=200 with 500 episodes,\n",
    " - and then train again with tmax = 400 with 500 episodes,\n",
    " - and then finally with a even larger tmax.\n",
    "- Remember to save your policy after training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never forget\n",
    "- keep your **gamma as high as possible**\n",
    "- **rarely decay** your **learning_rate** to 0\n",
    "- **never set epsilon = 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ca4812aec538>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# The corresponding snippet of code was as follows:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Then, the only change we made was to use gradient clipping when training the critic network.\n",
    "# The corresponding snippet of code was as follows:\n",
    "self.critic_optimizer.zero_grad()\n",
    "critic_loss.backward()\n",
    "torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we decided to get **less aggressive with the number of updates per time step**.\n",
    "In particular, instead of updating the actor and critic networks **20 times at every timestep**,\n",
    "we amended the code to update the networks **10 times after every 20 timesteps**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode 48\tAverage Score: 0.01\tScore: 0.00actions batch at 34000-th learning:\n",
    "# \t shape = (128, 4),\n",
    "# \t mean = [-0.98543936 -0.984659   -0.9865557  -0.9849621 ],\n",
    "# \t  std = [0.1161157  0.11468245 0.1066971  0.11414471]\n",
    "self.learning_counter += 1\n",
    "if self.learning_counter % 10 == 0:\n",
    "    info = \"actions batch at {}-th learning:\\n\\t shape = {},\\n\\t mean = {},\\n\\t  std = {}\".format(self.learning_counter, np.shape(actions.cpu().data.numpy()), actions.cpu().data.numpy().mean(0), actions.cpu().data.numpy().std(0))\n",
    "    print(info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
