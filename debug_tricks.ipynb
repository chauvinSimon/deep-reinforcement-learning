{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor progress - flush espilon\n",
    "print(\"\\rEpisode {}/{} || Best average reward {} - agent.epsilon = {}\".format(i_episode, num_episodes, best_avg_reward, agent.epsilon), end=\"\")\n",
    "sys.stdout.flush()\n",
    "# or\n",
    "print(\"\\r {}, {}, {}, {}, {}\".format(action, reward, done, min_reward, max_reward),sep=' ', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TD - avg_scores (window) with deque\n",
    "from collections import deque\n",
    "    # monitor performance\n",
    "    tmp_scores = deque(maxlen=plot_every)     # deque for keeping track of scores\n",
    "    avg_scores = deque(maxlen=num_episodes)   # average scores over every plot_every episodes\n",
    "    score = 0\n",
    "    \n",
    "    # in the loop\n",
    "    score += reward                                   # add reward to agent's score\n",
    "    # when terminated\n",
    "    tmp_scores.append(score)    # append score\n",
    "    if (i_episode % plot_every == 0):\n",
    "        avg_scores.append(np.mean(tmp_scores))\n",
    "\n",
    "# plot performance\n",
    "plt.plot(np.linspace(0,num_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "plt.show()\n",
    "# print best 100-episode performance\n",
    "print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Value Function\n",
    "# first convert Q to V\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])\n",
    "print(V_sarsa)\n",
    "# For a specific state (e.g. the one just before the end), we know the value (= Reward)\n",
    "expected_V_last_but_one = +10\n",
    "print(V_sarsa[last_but_one] vs expected_V_last_but_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 0: 3, -1: 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "actions = [1, 2, 0, 1, 0, 1]\n",
    "rewards = [+1, 0, 0, 0, 1, -1]\n",
    "state = np.array([0, 1, 2])\n",
    "next_state = np.array([0, 1, 0])\n",
    "changes_in_state = 0\n",
    "from collections import Counter\n",
    "# monitor if the agent moves\n",
    "if not (next_state == state).all():\n",
    "    changes_in_state = changes_in_state + 1  # changes_in_state: 300\n",
    "# monitor distribution in actions\n",
    "Counter(actions)  # action: Counter({1: 151, 0: 149}) shows tilts\n",
    "# monitor distribution in rewards\n",
    "Counter(rewards)  # rewards: Counter({0.0: 298, -1.0: 1, 1.0: 1}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have noticed it progressively learns to tilt\n",
    "# Clearly visible on the scene (it does not move)\n",
    "# And in the distribution of actions I record: \t action: Counter({1: 151, 0: 149})\n",
    "# Should not it be possible to mask action accordingly\n",
    "# For instance, from the beginning, disable the backward step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is the state built?\n",
    "# I understood some laser measurements are used to estimate distance in several directions.\n",
    "# But how (if applicable) are encoded the following aspects:\n",
    "# - Colour of the bananas?\n",
    "# - Previous action?\n",
    "# - Previous state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linspace() - not np.arange()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "eps_decay = 0.995\n",
    "nb_episodes = 1500\n",
    "eps_end = 0.01\n",
    "eps_start = 1.0\n",
    "eps = [max(eps_start * eps_decay ** i, eps_end) for i in range(nb_episodes)]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(eps)), eps)\n",
    "plt.plot(np.arange(len(eps)), np.linspace(eps_start, eps_end, len(eps)))\n",
    "plt.ylabel('Espilon')\n",
    "plt.xlabel('Episode #')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
