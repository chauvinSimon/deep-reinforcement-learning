{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walkthrough : https://www.youtube.com/watch?v=XhfhR7Z01S0\n",
    "        \n",
    "# Requirements:\n",
    "    # conda install -c anaconda cloudpickle \n",
    "    # conda install -c conda-forge jsanimation\n",
    "    # conda install -c anaconda progressbar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "# env = gym.make('Pong-v4')\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "RIGHT = 4\n",
    "LEFT = 5\n",
    "# do not learn to use FIRE. Directly use them\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "f0, _, _, _ = env.step(0)  # (210, 160, 3)\n",
    "print(\"original image has size = {}\".format(np.shape(f0)))\n",
    "\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(15, 15), dpi=80)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "bkg_color=np.array([144, 72, 17])\n",
    "tmp = frame[34:-16:2, ::2]-bkg_color\n",
    "plt.imshow(tmp)\n",
    "plt.title('tmp image')\n",
    "\n",
    "# original image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('preprocessed image')\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving right. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # input size = 80x80x2 (stacked frame)\n",
    "        \n",
    "        # stride = step size when moving the filter \"step by step\" along the picture\n",
    "        # stride == 2 -> every two pixels\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride\n",
    "        \n",
    "        # from 80x80x2 to 38x38x4\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=4,\n",
    "                               kernel_size=6, stride=2, bias=False)\n",
    "        \n",
    "        # from 38x38x4 to 9x9x16\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=16,\n",
    "                               kernel_size=6, stride=4)\n",
    "        self.size=9*9*16  # 9*9*16 = 1296\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # [1, 4, 38, 38]\n",
    "        x = F.relu(self.conv2(x))  # [1, 16, 9, 9]\n",
    "        x = x.view(-1, self.size)  # [1, 1296] - the size -1 is inferred from other dimensions296x=\n",
    "        x = F.relu(self.fc1(x))  # [1, 256]\n",
    "        return self.sig(self.fc2(x))  # [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your own policy!\n",
    "policy=Policy().to(device)\n",
    "# policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Game visualization - need JSAnimation\n",
    "pong_utils.play(env, policy, preprocess=pong_utils.preprocess_single, time=200)\n",
    "# pong_utils.play(env, policy, preprocess=None, time=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1, *states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - REINFORCE: write your own function for training\n",
    "(this is the same as policy_loss except the negative sign)\n",
    "\n",
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed):\n",
    "\n",
    "-   1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "-   2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division\n",
    "\n",
    "REINFORCE idea:\n",
    "-\t1. Collect experience from one policy\n",
    "-\t2. Estimate the gradient of that policy (Theorem)\n",
    "-\t3. Gradient ascent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount=0.995, beta=0.01):\n",
    "    \"\"\"\n",
    "    return sum of log-prob divided by T\n",
    "    same thing as minus*policy_loss\n",
    "    \"\"\"\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:, np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:, np.newaxis])/std[:, np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10) +\n",
    "                (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(ratio*rewards + beta*entropy)\n",
    "#     return torch.mean(beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "envs = parallelEnv('PongDeterministic-v4', n=4, seed=12345)\n",
    "prob, state, action, reward = pong_utils.collect_trajectories(envs, policy, t_max=100)\n",
    "Lsur = surrogate(policy, prob, state, action, reward)\n",
    "print(Lsur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - PPO: write your own function for training\n",
    "(what I call scalar function is the same as policy_loss up to a negative sign)\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition:\n",
    "\n",
    "- PPO = a **family** of policy optimization methods that use **multiple epochs** of **stochastic gradient ascent** to perform each policy update\n",
    "- offers the stability and reliability of *trust-region methods* but are **much simpler to implement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Algorithm:\n",
    "- 1- First, collect some trajectories based on some policy $\\pi_{\\theta}$, and initialize theta prime $\\theta'=\\theta$\n",
    "- 2- Next, compute the gradient of the clipped surrogate function using the trajectories\n",
    "- 3- Update $\\theta'$ using gradient ascent $\\theta'\\leftarrow\\theta' +\\alpha \\nabla_{\\theta'}L_{\\rm sur}^{\\rm clip}(\\theta', \\theta)$\n",
    "- 4- Then we repeat step 2-3 without generating new trajectories. Typically, step 2-3 are only repeated a few times\n",
    "- 5- Set $\\theta=\\theta'$, go back to step 1, repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      entropies=[],\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "    \"\"\"\n",
    "    clipped surrogate function (the objective to maximize)\n",
    "    similar as minus*policy_loss\n",
    "    \"\"\"\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:, np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    # normalization against shift in distribution of rewards \n",
    "    rewards_normalized = (rewards_future - mean[:, np.newaxis])/std[:, np.newaxis]\n",
    "    # if reward = [1, 2, 1], then rewards_normalized has 3 rows like [-0.70  1.41 -0.70]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    # removes the incentive for moving rt outside of the interval [1 − e, 1 + e]\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    # the final objective is a lower bound - pessimistic:\n",
    "    #  - ignore the change in probability ratio when it would make the objective improve\n",
    "    #  - include it when it makes the objective worse\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term to force the policy to not be exactly equal to 0 or 1\n",
    "    # this steers new_policy towards 0.5\n",
    "    # also add in 1.e-10 to avoid log(0) which gives nan\n",
    "    #     Actually, it is the cross-entropy\n",
    "    # That is how \"far away\" predictions (old_probs) are from the true distribution (new_probs)\n",
    "    # H(p,q) = D_KL(p||q) + H(p)\n",
    "    # if new_probs == old_probs, then H(p,q)=H(p)\n",
    "    # TRYING WITH +\n",
    "    entropy = (new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    # simply normal entropy term + a bit of penalty from KL-divergence.\n",
    "    # the straight-up entropy term would be the clearest as it pushes policies to 0.5.\n",
    "    \n",
    "    entropies.append(np.mean(entropy.cpu().data.numpy()))\n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "envs = parallelEnv('PongDeterministic-v4', n=4, seed=12345)\n",
    "prob, state, action, reward = pong_utils.collect_trajectories(\n",
    "    envs, policy, t_max=10)\n",
    "# for 4 parallel agents, if t_max=10, then prob has size (4*10)\n",
    "# And L is the average of this table: L = 0.0011\n",
    "L_clip_sur = clipped_surrogate(policy, prob, state, action, reward)\n",
    "print(L_clip_sur.cpu().data.numpy())\n",
    "print(float(L_clip_sur.cpu().data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop max iterations\n",
    "episode = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widget bar to display progress\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel environment to collect multiple trajectories in a row\n",
    "# since GPU mode has 4 parallel virtual GPU, chose a multiple of 4 \n",
    "envs = parallelEnv('PongDeterministic-v4', n=4, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "probs, states, actions, rewards = pong_utils.collect_trajectories(envs, policy, t_max=2)\n",
    "print(\"probs = {}\".format(probs))\n",
    "print(\"state.size() of 1st state = {}\".format((states[0]).size()))\n",
    "print(\"actions = {}\".format(actions))\n",
    "print(\"rewards = {}\".format(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = \"REINFORCED\"\n",
    "method = \"PPO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For interpretation and setting of PPO paramters, refer to\n",
    "https://github.com/llSourcell/Unity_ML_Agents/blob/master/docs/best-practices-ppo.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common to PPO and REINFORCE\n",
    "discount_rate = .99\n",
    "epsilon = 0.1   # for clipping - removes the incentive for moving rt outside of [1 − e, 1 + e]\n",
    "# acceptable threshold of divergence between the old and new policies during GD updating\n",
    "# small epsilon will result in more stable updates, but will also slow the training process.\n",
    "t_max = 320  # to have trajectories that are smaller than one full episode\n",
    "# how many steps of experience to collect per-agent before adding it to the experience buffer\n",
    "\n",
    "# batch_size: how many experiences are used for each gradient descent update\n",
    "# should always be a fraction of the buffer_size\n",
    "# here, we do not sample from a buffer. We use all the available trajectories\n",
    "# hence batch_size = buffer_size = t_max*n_agents \n",
    "# for 4 agents and t_max=10, prob, entropy and clipped_surrogate have size (4*10)\n",
    "\n",
    "# specific to PPO\n",
    "beta = .01  # strength of the entropy regularization -- makes the policy \"more random\"\n",
    "# only for Discrete Control\n",
    "# beta be adjusted such that the entropy slowly decreases alongside increases in reward\n",
    "SGD_epoch = 4  # to reuse trajectories\n",
    "# number of passes through the experience buffer during gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of progress - book-keeping\n",
    "mean_rewards = []\n",
    "entropies = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where do we set θ′=θ ??\n",
    "for e in range(episode):\n",
    "    # first, collect some trajectories based on some policy πθ\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, t_max=t_max)\n",
    "\n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    if method == \"REINFORCE\":\n",
    "        # next, compute the gradient of the (clipped) surrogate function using the trajectories\n",
    "        L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "        # Update θ′ using gradient ascent\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    elif method == \"PPO\":\n",
    "        # repeat step \"compute the gradient\" and \"Update θ′\" without generating new trajectories\n",
    "        for _ in range(SGD_epoch):\n",
    "\n",
    "            # Update θ′ using gradient ascent\n",
    "            # pytorch by default does gradient DESCENT. Hence minus term\n",
    "            L = -clipped_surrogate(policy, old_probs, states,\n",
    "                                   actions, rewards,\n",
    "                                   entropies,\n",
    "                                   epsilon=epsilon, beta=beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(((L.cpu().data.numpy()).tolist()))\n",
    "#             print(losses)\n",
    "            del L\n",
    "\n",
    "    # the clipping parameter (PPO only) reduces as time goes on\n",
    "    epsilon *= .999\n",
    "\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta *= .995\n",
    "\n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "\n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1) % 20 == 0:\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(\n",
    "            e+1, np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "\n",
    "    if (e+1) % 50 == 0:\n",
    "        # save the policy\n",
    "        if method == \"REINFORCE\":\n",
    "            torch.save(policy, 'REINFORCE.policy')\n",
    "        elif method == \"PPO\":\n",
    "            torch.save(policy, 'PPO.policy')\n",
    "        with open(\"rewards.txt\", \"wb\") as fp:\n",
    "            pickle.dump(mean_rewards, fp)\n",
    "        with open(\"entropies.txt\", \"wb\") as fp:\n",
    "            pickle.dump(entropies, fp) \n",
    "        with open(\"losses.txt\", \"wb\") as fp:\n",
    "            pickle.dump(losses, fp)\n",
    "\n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "\n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Test the new policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Plot evolutions of the entropy, the rewards, the averaged rewards and the loss terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"entropies_pos_ent.txt\", \"rb\") as fp:\n",
    "    entropies = pickle.load(fp)\n",
    "    print(np.mean(entropies))\n",
    "    print(len(entropies))\n",
    "\n",
    "with open(\"rewards_pos_ent.txt\", \"rb\") as fp:\n",
    "    mean_rewards = pickle.load(fp)\n",
    "    print(np.mean(mean_rewards))\n",
    "    print(len(mean_rewards))\n",
    "\n",
    "with open(\"losses_pos_ent.txt\", \"rb\") as fp:\n",
    "    losses = pickle.load(fp)\n",
    "    print(np.mean(losses))\n",
    "    print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / N\n",
    "window_size = 100\n",
    "threshold = -5\n",
    "\n",
    "avg_mean_rewards = running_mean(mean_rewards, window_size)\n",
    "success_index = next(x[0] for x in enumerate(\n",
    "    avg_mean_rewards) if x[1] > threshold)\n",
    "title = \"Solved in {} episodes (={}*{} updates) -- for window_size = {} and threshold = {}\".format(\n",
    "    success_index, SGD_epoch, success_index, window_size, threshold)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand mean_rewards from (for each episode) to (for each SGD_epoch)\n",
    "mean_rewards = [[e]*SGD_epoch for e in mean_rewards]\n",
    "mean_rewards = [item for sublist in mean_rewards for item in sublist]\n",
    "# padding for plotting\n",
    "avg_mean_rewards = running_mean(mean_rewards, window_size)\n",
    "avg_mean_rewards = list(avg_mean_rewards)\n",
    "avg_mean_rewards = [avg_mean_rewards[0]] * (len(entropies) - len(avg_mean_rewards)) + avg_mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(losses)+1)\n",
    "print(len(mean_rewards)+1)\n",
    "print(len(avg_mean_rewards)+1)\n",
    "print(len(entropies)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "# choose if you want to use the raw mean_rewards or its moving average\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig = plt.figure(figsize=(18, 16), dpi=80, facecolor='w', edgecolor='k')\n",
    "host = fig.add_subplot(111)\n",
    "\n",
    "par1 = host.twinx()\n",
    "par2 = host.twinx()\n",
    "\n",
    "# host.set_xlim(0, 2)\n",
    "# host.set_ylim(0, 2)\n",
    "# par1.set_ylim(0, 4)\n",
    "# par2.set_ylim(1, 65)\n",
    "\n",
    "host.set_xlabel(\"# update (= \" + str(SGD_epoch) + \" per episode)\", fontsize=20)\n",
    "# host.set_ylabel(\"mean_reward\", fontsize=20)\n",
    "host.set_ylabel(\"avg_mean_reward\", fontsize=20)\n",
    "par1.set_ylabel(\"losses\", fontsize=20)\n",
    "par2.set_ylabel(\"entropy\", fontsize=20)\n",
    "\n",
    "color1 = plt.cm.viridis(0)\n",
    "color2 = plt.cm.viridis(0.5)\n",
    "color3 = plt.cm.viridis(.9)\n",
    "\n",
    "# p1, = host.plot(np.arange(1, len(mean_rewards)+1), mean_rewards, color=color1, label=\"mean_rewards\")\n",
    "p1, = host.plot(np.arange(1, len(avg_mean_rewards)+1), np.asarray(avg_mean_rewards), color=color1, label=\"avg_mean_rewards\")\n",
    "p2, = par1.plot(np.arange(1, len(losses)+1), losses, color=color2, label=\"losses\", zorder=5)\n",
    "p3, = par2.plot(np.arange(1, len(entropies)+1), entropies, color=color3, label=\"entropy\")\n",
    "p4 = host.axhline(y=threshold, c=\"r\", label=\"success criteria\", linestyle=\"--\", linewidth=3)\n",
    "\n",
    "lns = [p1, p2, p3, p4]\n",
    "labs = [l.get_label() for l in lns]\n",
    "par2.spines['right'].set_position(('outward', 120))      \n",
    "\n",
    "# text sizes\n",
    "host.legend(lns, labs, prop={'size': 20}, bbox_to_anchor=(1.17, 1), borderaxespad=0.)\n",
    "host.xaxis.set_tick_params(labelsize=20)\n",
    "host.yaxis.set_tick_params(labelsize=20)\n",
    "par1.yaxis.set_tick_params(labelsize=20)\n",
    "par2.yaxis.set_tick_params(labelsize=20)\n",
    "\n",
    "# text color\n",
    "host.yaxis.label.set_color(p1.get_color())\n",
    "par1.yaxis.label.set_color(p2.get_color())\n",
    "par2.yaxis.label.set_color(p3.get_color())\n",
    "\n",
    "# title\n",
    "duration = 10\n",
    "plt.title(title, fontsize=20)\n",
    "\n",
    "host.grid()\n",
    "# plt.savefig(\"training_tmp.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the policy\n",
    "if method == \"REINFORCE\":\n",
    "    torch.save(policy, 'REINFORCE_solution.policy')\n",
    "elif method == \"PPO\":\n",
    "    torch.save(policy, 'PPO_solution.policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mind the method\n",
    "policy = torch.load('PPO_BU.policy')\n",
    "# policy = torch.load('REINFORCE.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
