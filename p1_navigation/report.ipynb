{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "---\n",
    "To describe choices and detail results. It includes\n",
    "- Description of the **model architectures** \n",
    "- Description of the **hyperparameters**\n",
    "- Plot of **Rewards**\n",
    "- Ideas for **Future Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the model architectures \n",
    "The repository is structured as follow.\n",
    "- [`main_banana.ipynb`](src_submission/main_banana.ipynb) is *the central file you want to use*. It contains\n",
    "    - all the import statements and instructions to start the environment\n",
    "    - the calls to *train*\n",
    "    - the calls to *test*\n",
    "- [`dqn_agent_banana.py`](src_submission/dqn_agent_banana.py) defines two classes\n",
    "    - Agent with methods such as step, act, learn \n",
    "    - ReplayBuffer to store experience tuples \n",
    "- [`model_banana.py`](src_submission/model_banana.py) defines the Q-Network used by the Agent as a function approximation for Q-values\n",
    "- [`checkpoint_banana_431.pth`](src_submission/checkpoint_banana_431.pth) are the saved model weights of the successful agent weights\n",
    "\n",
    "Since the state space is relatively small (compared to 84 * 84 RGB pixels), there is no need to implement convolutional layers. Multiple **fully-connected units** show good results.\n",
    "\n",
    "The architecture of the Q-network is composed of\n",
    "- State (space size = 37)\n",
    "- Fully-connected layer with **64** outputs\n",
    "- **ReLu** activation function\n",
    "- Fully-connected layer with **64** outputs\n",
    "- **ReLu** activation function\n",
    "- Fully-connected layer with **4** outputs (= action space size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, every **return** (=sum of collected rewards) is saved. At the end, a plot shows their evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![caption](report_submission/431/431.svg) | \n",
    "|:--:| \n",
    "| *Rewards for each episode* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One could try to modify the environment, especially to add **different colours** with **different rewards**\n",
    "    -  this [link](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md) shows how to build Unity environments\n",
    "    \n",
    "- It would be interesting to add a **energy-constraint** on the agent in the form of a **limited battery life**.\n",
    "    - the goal would be to collect bananas while **taking care not to run out of energy**\n",
    "    - one or **multiple loading stations** could be placed in the environment\n",
    "    - when running low, the decision would be to **stop chasing bananas** to refuel the battery\n",
    "    - a **hierarchical strategy** could be implemented.\n",
    "    - Approaches such as thoses described [here](https://www.oreilly.com/ideas/reinforcement-learning-for-complex-goals-using-tensorflow) for **“direct future prediction” (DFP)** also looks promizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![caption](https://cdn-images-1.medium.com/max/1600/1*EmYiXaxLGxvLXFSquBFGrQ.png) | \n",
    "|:--:| \n",
    "| *Possible states of the robot, transitional probabilities and respective rewards. (Source: Udacity)* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
